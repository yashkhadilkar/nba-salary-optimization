{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac550024-2ead-4928-8b7e-7ee2b41ad0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBA Salary Optimization - Data Collection\n",
      "==================================================\n",
      "Notebook created: 2025-07-14 21:31:46\n",
      "Python environment: nba-analysis\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"NBA Salary Optimization - Data Collection\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Notebook created: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Python environment: nba-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f500941-63e4-47d7-952e-7b7025bceec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Testing Basketball Reference...\n",
      "‚úÖ Basketball Reference: Connection successful\n",
      "‚úÖ Found stats table with 737 rows\n",
      "üìä Sample players found:\n"
     ]
    }
   ],
   "source": [
    "def test_basketball_reference():\n",
    "    \"\"\"Test connection to Basketball Reference\"\"\"\n",
    "    print(\"\\nüîç Testing Basketball Reference...\")\n",
    "    \n",
    "    try:\n",
    "        url = \"https://www.basketball-reference.com/leagues/NBA_2024_totals.html\"\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"‚úÖ Basketball Reference: Connection successful\")\n",
    "            \n",
    "            # Parse the page\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            table = soup.find('table', {'id': 'totals_stats'})\n",
    "            \n",
    "            if table:\n",
    "                rows = table.find_all('tr')\n",
    "                print(f\"‚úÖ Found stats table with {len(rows)} rows\")\n",
    "                \n",
    "                # Get first few player names as test\n",
    "                player_rows = [row for row in rows if row.find('td')][:3]\n",
    "                print(\"üìä Sample players found:\")\n",
    "                for row in player_rows:\n",
    "                    player_cell = row.find('td', {'data-stat': 'player'})\n",
    "                    if player_cell:\n",
    "                        player_name = player_cell.text.strip()\n",
    "                        print(f\"   ‚Ä¢ {player_name}\")\n",
    "                \n",
    "                return True\n",
    "            else:\n",
    "                print(\"‚ùå Could not find stats table\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"‚ùå Basketball Reference: HTTP {response.status_code}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Basketball Reference: Error - {e}\")\n",
    "        return False\n",
    "\n",
    "# Run the test\n",
    "br_success = test_basketball_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64f20d60-69a5-4882-8237-4b4af05df5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí∞ Testing Salary Data Sources...\n",
      "‚úÖ Salary data: Connection successful\n",
      "‚úÖ Found salary table with 407 rows\n",
      "üíµ Sample salary data:\n",
      "   ‚Ä¢ Stephen Curry: GSW\n",
      "   ‚Ä¢ Joel Embiid: PHI\n",
      "   ‚Ä¢ Nikola Jokiƒá: DEN\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Test salary data source (UPDATED)\n",
    "def test_salary_data():\n",
    "    \"\"\"Test salary data sources\"\"\"\n",
    "    print(\"\\nüí∞ Testing Salary Data Sources...\")\n",
    "    \n",
    "    try:\n",
    "        # Test Basketball Reference salary page\n",
    "        url = \"https://www.basketball-reference.com/contracts/players.html\"\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"‚úÖ Salary data: Connection successful\")\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Try multiple possible table IDs/classes\n",
    "            table = soup.find('table', {'id': 'contracts'})\n",
    "            if not table:\n",
    "                table = soup.find('table', {'class': 'sortable'})\n",
    "            if not table:\n",
    "                table = soup.find('table')  # Get any table\n",
    "            \n",
    "            if table:\n",
    "                rows = table.find_all('tr')\n",
    "                print(f\"‚úÖ Found salary table with {len(rows)} rows\")\n",
    "                \n",
    "                # Get sample salary data\n",
    "                data_rows = [row for row in rows if len(row.find_all('td')) > 1][:3]\n",
    "                if data_rows:\n",
    "                    print(\"üíµ Sample salary data:\")\n",
    "                    for row in data_rows:\n",
    "                        cells = row.find_all('td')\n",
    "                        if len(cells) >= 2:\n",
    "                            player = cells[0].text.strip()\n",
    "                            salary_info = cells[1].text.strip()\n",
    "                            print(f\"   ‚Ä¢ {player}: {salary_info}\")\n",
    "                else:\n",
    "                    print(\"üíµ Table found but data structure different (will handle in collection)\")\n",
    "                \n",
    "                return True\n",
    "            else:\n",
    "                print(\"‚ùå Could not find any salary table\")\n",
    "                print(\"üí° Will try alternative salary sources tomorrow\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"‚ùå Salary data: HTTP {response.status_code}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Salary data: Error - {e}\")\n",
    "        return False\n",
    "\n",
    "# Wait a moment to be respectful to servers\n",
    "time.sleep(2)\n",
    "salary_success = test_salary_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78886432-01b0-4ae9-8072-1e61a4cd1360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Testing Additional Data Sources...\n",
      "‚úÖ ESPN API: Found 30 teams\n",
      "‚ö†Ô∏è NBA.com Stats: HTTP 500 (expected - they have rate limiting)\n"
     ]
    }
   ],
   "source": [
    "def test_additional_sources():\n",
    "    \"\"\"Test other potential data sources\"\"\"\n",
    "    print(\"\\nüîç Testing Additional Data Sources...\")\n",
    "    \n",
    "    sources_tested = []\n",
    "    \n",
    "    # Test ESPN (simpler endpoint)\n",
    "    try:\n",
    "        url = \"https://site.api.espn.com/apis/site/v2/sports/basketball/nba/teams\"\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            teams = data.get('sports', [{}])[0].get('leagues', [{}])[0].get('teams', [])\n",
    "            print(f\"‚úÖ ESPN API: Found {len(teams)} teams\")\n",
    "            sources_tested.append(\"ESPN API\")\n",
    "        else:\n",
    "            print(f\"‚ùå ESPN API: HTTP {response.status_code}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ESPN API: Error - {e}\")\n",
    "    \n",
    "    # Test if we can access NBA stats (often rate limited)\n",
    "    try:\n",
    "        url = \"https://stats.nba.com/stats/leagueleaders\"\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "            'Referer': 'https://stats.nba.com/',\n",
    "            'Origin': 'https://stats.nba.com'\n",
    "        }\n",
    "        \n",
    "        # This might fail due to rate limiting - that's expected\n",
    "        response = requests.get(url, headers=headers, timeout=5)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"‚úÖ NBA.com Stats: Accessible\")\n",
    "            sources_tested.append(\"NBA.com Stats\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è NBA.com Stats: HTTP {response.status_code} (expected - they have rate limiting)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è NBA.com Stats: {str(e)[:50]}... (expected - they have strict rate limiting)\")\n",
    "    \n",
    "    return sources_tested\n",
    "\n",
    "additional_sources = test_additional_sources()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e670762-7eb3-49b7-8923-92be84006df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìã DATA SOURCE TESTING SUMMARY\n",
      "============================================================\n",
      "\n",
      "‚úÖ Basketball Reference Player Stats: Ready\n",
      "‚úÖ Basketball Reference Salary Data: Ready\n",
      "‚úÖ Additional Sources Available: ESPN API\n",
      "\n",
      "üéØ PRIMARY DATA STRATEGY:\n",
      "   ‚Ä¢ Basketball Reference will be our main source\n",
      "   ‚Ä¢ Reliable for both stats and salary data\n",
      "   ‚Ä¢ Historical data available (2019-2024)\n",
      "\n",
      "üéâ STATUS: Ready for data collection!\n",
      "‚úÖ Day 1 Complete - All systems go!\n"
     ]
    }
   ],
   "source": [
    "def print_summary():\n",
    "    \"\"\"Print summary of data source testing\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìã DATA SOURCE TESTING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Basketball Reference Player Stats: {'Ready' if br_success else 'Issues'}\")\n",
    "    print(f\"‚úÖ Basketball Reference Salary Data: {'Ready' if salary_success else 'Issues'}\")\n",
    "    \n",
    "    if additional_sources:\n",
    "        print(f\"‚úÖ Additional Sources Available: {', '.join(additional_sources)}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Additional Sources: Limited (normal for public APIs)\")\n",
    "    \n",
    "    print(f\"\\nüéØ PRIMARY DATA STRATEGY:\")\n",
    "    print(f\"   ‚Ä¢ Basketball Reference will be our main source\")\n",
    "    print(f\"   ‚Ä¢ Reliable for both stats and salary data\")\n",
    "    print(f\"   ‚Ä¢ Historical data available (2019-2024)\")\n",
    "    \n",
    "    if br_success and salary_success:\n",
    "        print(f\"\\nüéâ STATUS: Ready for data collection!\")\n",
    "        print(f\"‚úÖ Day 1 Complete - All systems go!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è STATUS: Some issues detected\")\n",
    "        print(f\"üí° Next steps: Check network connection and retry\")\n",
    "\n",
    "print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97744745-c143-4d70-b107-2c0aa2c61db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìÖ DAY 2 PREVIEW: Data Collection Plan\n",
      "============================================================\n",
      "\n",
      "üìä Player Stats (2019-2024):\n",
      "   ‚Ä¢ Basic stats: Points, Rebounds, Assists, Games\n",
      "   ‚Ä¢ Shooting: FG%, 3P%, FT%, True Shooting%\n",
      "   ‚Ä¢ Advanced: PER, VORP, BPM, Win Shares\n",
      "   ‚Ä¢ Usage metrics: Usage Rate, PIE, Minutes\n",
      "\n",
      "üìä Salary Data (2019-2024):\n",
      "   ‚Ä¢ Current season salary by player\n",
      "   ‚Ä¢ Contract length and total value\n",
      "   ‚Ä¢ Cap hit and luxury tax implications\n",
      "   ‚Ä¢ Team salary distributions\n",
      "\n",
      "üìä Team Performance:\n",
      "   ‚Ä¢ Win-loss records by season\n",
      "   ‚Ä¢ Playoff performance\n",
      "   ‚Ä¢ Team efficiency metrics\n",
      "   ‚Ä¢ Salary cap utilization\n",
      "\n",
      "üìà EXPECTED DATA VOLUME:\n",
      "   ‚Ä¢ Total records: ~5,150\n",
      "   ‚Ä¢ Estimated size: 20-50MB\n",
      "   ‚Ä¢ Format: CSV files + cleaned datasets\n",
      "\n",
      "üõ†Ô∏è TOMORROW'S TASKS:\n",
      "   1. Build web scraping functions\n",
      "   2. Collect 2024 season data first (test)\n",
      "   3. Scale to historical seasons (2019-2023)\n",
      "   4. Data validation and quality checks\n",
      "   5. Export clean datasets for analysis\n"
     ]
    }
   ],
   "source": [
    "def preview_collection_plan():\n",
    "    \"\"\"Preview what data we'll collect tomorrow\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìÖ DAY 2 PREVIEW: Data Collection Plan\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    collection_plan = {\n",
    "        \"Player Stats (2019-2024)\": [\n",
    "            \"Basic stats: Points, Rebounds, Assists, Games\",\n",
    "            \"Shooting: FG%, 3P%, FT%, True Shooting%\",\n",
    "            \"Advanced: PER, VORP, BPM, Win Shares\",\n",
    "            \"Usage metrics: Usage Rate, PIE, Minutes\"\n",
    "        ],\n",
    "        \"Salary Data (2019-2024)\": [\n",
    "            \"Current season salary by player\",\n",
    "            \"Contract length and total value\",\n",
    "            \"Cap hit and luxury tax implications\",\n",
    "            \"Team salary distributions\"\n",
    "        ],\n",
    "        \"Team Performance\": [\n",
    "            \"Win-loss records by season\",\n",
    "            \"Playoff performance\",\n",
    "            \"Team efficiency metrics\",\n",
    "            \"Salary cap utilization\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    total_expected = 0\n",
    "    for category, items in collection_plan.items():\n",
    "        print(f\"\\nüìä {category}:\")\n",
    "        for item in items:\n",
    "            print(f\"   ‚Ä¢ {item}\")\n",
    "        \n",
    "        if \"Player\" in category:\n",
    "            total_expected += 500 * 5  # ~500 players √ó 5 seasons\n",
    "        elif \"Salary\" in category:\n",
    "            total_expected += 500 * 5\n",
    "        else:\n",
    "            total_expected += 30 * 5  # 30 teams √ó 5 seasons\n",
    "    \n",
    "    print(f\"\\nüìà EXPECTED DATA VOLUME:\")\n",
    "    print(f\"   ‚Ä¢ Total records: ~{total_expected:,}\")\n",
    "    print(f\"   ‚Ä¢ Estimated size: 20-50MB\")\n",
    "    print(f\"   ‚Ä¢ Format: CSV files + cleaned datasets\")\n",
    "    \n",
    "    print(f\"\\nüõ†Ô∏è TOMORROW'S TASKS:\")\n",
    "    print(f\"   1. Build web scraping functions\")\n",
    "    print(f\"   2. Collect 2024 season data first (test)\")\n",
    "    print(f\"   3. Scale to historical seasons (2019-2023)\")\n",
    "    print(f\"   4. Data validation and quality checks\")\n",
    "    print(f\"   5. Export clean datasets for analysis\")\n",
    "\n",
    "preview_collection_plan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d6029a9-3370-4129-bd21-b99e16720403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üîß ENVIRONMENT VERIFICATION\n",
      "============================================================\n",
      "\n",
      "üì¶ Key Libraries:\n",
      "   ‚úÖ pandas: v2.0.3\n",
      "   ‚úÖ numpy: v1.24.3\n",
      "   ‚úÖ requests: v2.32.3\n",
      "   ‚úÖ beautifulsoup4: Installed\n",
      "\n",
      "üìÅ Project Structure:\n",
      "   ‚ö†Ô∏è ../data/raw: Missing (will create tomorrow)\n",
      "   ‚ö†Ô∏è ../data/processed: Missing (will create tomorrow)\n",
      "   ‚ö†Ô∏è ../src: Missing (will create tomorrow)\n",
      "   ‚ö†Ô∏è ../results: Missing (will create tomorrow)\n",
      "\n",
      "üéØ READY FOR DAY 2!\n",
      "   Next notebook: 02_data_cleaning_eda.ipynb\n",
      "   Focus: Actual data collection and initial analysis\n"
     ]
    }
   ],
   "source": [
    "def verify_environment():\n",
    "    \"\"\"Final verification that everything is ready\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üîß ENVIRONMENT VERIFICATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Check key libraries\n",
    "    libraries = {\n",
    "        'pandas': pd.__version__,\n",
    "        'numpy': np.__version__,\n",
    "        'requests': requests.__version__,\n",
    "        'beautifulsoup4': BeautifulSoup.__module__\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüì¶ Key Libraries:\")\n",
    "    for lib, version in libraries.items():\n",
    "        if 'bs4' in str(version):\n",
    "            print(f\"   ‚úÖ {lib}: Installed\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ {lib}: v{version}\")\n",
    "    \n",
    "    # Check data directories\n",
    "    import os\n",
    "    required_dirs = ['../data/raw', '../data/processed', '../src', '../results']\n",
    "    \n",
    "    print(f\"\\nüìÅ Project Structure:\")\n",
    "    for directory in required_dirs:\n",
    "        if os.path.exists(directory):\n",
    "            print(f\"   ‚úÖ {directory}: Exists\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è {directory}: Missing (will create tomorrow)\")\n",
    "    \n",
    "    print(f\"\\nüéØ READY FOR DAY 2!\")\n",
    "    print(f\"   Next notebook: 02_data_cleaning_eda.ipynb\")\n",
    "    print(f\"   Focus: Actual data collection and initial analysis\")\n",
    "\n",
    "verify_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "583b7434-c2c3-48a6-887e-f8068b56c374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Progress logged (file save skipped - directory may not exist yet)\n",
      "\n",
      "üèÜ Day 1 Status: COMPLETE!\n",
      "üöÄ Ready to start serious data collection tomorrow!\n"
     ]
    }
   ],
   "source": [
    "def save_checkpoint():\n",
    "    \"\"\"Save our progress\"\"\"\n",
    "    checkpoint_data = {\n",
    "        'day_1_complete': True,\n",
    "        'basketball_reference_working': br_success,\n",
    "        'salary_data_working': salary_success,\n",
    "        'additional_sources': additional_sources,\n",
    "        'timestamp': pd.Timestamp.now().isoformat(),\n",
    "        'next_tasks': [\n",
    "            'Build web scraping functions',\n",
    "            'Collect 2024 season data',\n",
    "            'Expand to historical seasons',\n",
    "            'Data validation and cleaning'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Create a simple progress file\n",
    "    import json\n",
    "    \n",
    "    try:\n",
    "        with open('../data/day1_checkpoint.json', 'w') as f:\n",
    "            json.dump(checkpoint_data, f, indent=2)\n",
    "        print(\"\\nüíæ Progress saved to data/day1_checkpoint.json\")\n",
    "    except:\n",
    "        print(\"\\nüíæ Progress logged (file save skipped - directory may not exist yet)\")\n",
    "    \n",
    "    print(f\"\\nüèÜ Day 1 Status: COMPLETE!\")\n",
    "    print(f\"üöÄ Ready to start serious data collection tomorrow!\")\n",
    "\n",
    "save_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751b476f-f641-4ffc-89ea-ed50a9d67ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
